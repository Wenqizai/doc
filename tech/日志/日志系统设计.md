1. 技术选型
before: elk + kinaba, 内存贵
after: lk + doris 


OLAP 实时统计数据库


ELK stack 中文指南：[hello world · ELKstack 中文指南](https://elkguide.elasticsearch.cn/logstash/get-start/hello-world.html)

[Spring Cloud 系列之 Sleuth 链路追踪（一） - 哈喽沃德先生 - 博客园](https://www.cnblogs.com/mrhelloworld/p/sleuth1.html)

[Spring Cloud Sleuth 2.0概要使用说明 - BTStream's Blog](https://blog.btstream.net/post/2019-01-14-spring-cloud-sleuth-2.0%E6%A6%82%E8%A6%81%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E/)

基于 clickhouse 的日志系统

[JLog: 来自京东App秒级百G级日志搜集、传输、存储解决方案](https://gitee.com/jd-platform-opensource/jlog?_from=gitee_search)


> 思想

[51 分布式下如何实现统一日志系统？](https://learn.lianglianglee.com/%E4%B8%93%E6%A0%8F/%E5%88%86%E5%B8%83%E5%BC%8F%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%9845%E8%AE%B2-%E5%AE%8C/51%20%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%8B%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E7%BB%9F%E4%B8%80%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9F%EF%BC%9F.md)

[性能调优之日志打印的坑 - 楼兰胡杨 - 博客园](https://www.cnblogs.com/east7/p/18013052)

> 工具

正则表达式练习：[ttps://regexlearn.com/](https://regexlearn.com/)


# logback
## 相关文档

官方文档：[Documentation](https://logback.qos.ch/documentation.html)
中文文档：  [README | logback](https://logbackcn.gitbook.io/logback)



> 一些坑 

1. 应用服务内压缩日志导致 cpu 过高问题: 不应该在应用服务内执行压缩, 应该将日志文件转移至其他空闲服务器执行压缩.

[技术干货|关于logback日志压缩的那点事 - 知乎](https://zhuanlan.zhihu.com/p/133425425)

## 架构

日志级别：**TRACE** < **DEBUG** < **INFO** < **WARN** < **ERROR**。

⚠️upload failed, check dev console
![[Logback 日志级别.png]]

![Logback 日志级别.png](%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1.assets/Logback%20%E6%97%A5%E5%BF%97%E7%BA%A7%E5%88%AB.png)

流程

1. 定义 `logback.xml`，维护 root logger，自定义 Appender，filter，level；
2. 获取 logger，`Logger logger = LoggerFactory.getLogger("className or loggerName");`，如果没有找到自定义的 logger，一直往父级查找，直到 root logger，查找到第一个 logger 为止。
3. 使用获取到的 logger，判断日志的优先级时候执行，获取 logger 绑定的 Appender，执行 `doAppend()` 方法，打印日志。


参考文档：[打印日志时 Logback 内部都做了些什么](https://tech.youzan.com/how-logback-print-log/)
## 注意点
1. 定义多个 root 节点

`logback.xml` 中定义多个 root 节点时，后面的 root 节点定义会覆盖前面的定义。具体还需要看解析器的执行。而且这种做法是不规范，会导致 appender 绑定混乱。标准做法是只定义一个 root 节点。


2. 配置 additivity = false

- 子 logger 会默认继承父 logger 的 appender，将它们加入到自己的 Appender 中；除非加上了**additivity="false"**，则不再继承父 logger 的 appender。
- 子 logger 只在自己未定义输出级别的情况下，才会继承父 logger 的输出级别。
- 【**强制】** 避免重复打印日志，浪费磁盘空间，务必在日志配置文件中设置 additivity=false


# Log4j2

官方文档：[Log4j – Overview](https://logging.apache.org/log4j/2.x/manual/index.html)
中文文档：[Log4j2 中文文档 - 欢迎使用 Log4j 2！ | Docs4dev](https://www.docs4dev.com/docs/zh/log4j2/2.x/all/manual-index.html)
github: [GitHub - apache/logging-log4j2: Apache Log4j 2 is a versatile, feature-rich, efficient logging API and backend for Java.](https://github.com/apache/logging-log4j2)

## 博客

1. 日志的执行流程，异步的无锁数据结构 Disruptor
[Log4j2中的同步日志与异步日志 - Ye\_yang - 博客园](https://www.cnblogs.com/yeyang/p/7944906.html)

![[Log4j2日志输出方式同步和异步.png]]



## API

> 获取 logger
-  `org.apache.logging.log4j.spi.AbstractLoggerAdapter#getLogger`

>构建 log 消息
- `org.apache.logging.log4j.message.MessageFactory2`
- `org.apache.logging.log4j.message.MessageFactory`

## 使用

> 静态还是非静态

建议使用静态的 Logger，因为创建 Logger 很昂贵，而 Logger 被关联的 LoggerContext 关联，不会将其回收。
[Log4j2 中文文档 - Usage | Docs4dev](https://www.docs4dev.com/docs/zh/log4j2/2.x/all/manual-usage.html)


> 关于性能

![[log4j2打印消耗性能的操作.png]]



# Logstash

## 架构
### 数据流

注意数据流程结构：`input | decode | filter | encode | output`。

> 疑问:

1. Exception, run, request, response log 输出的格式不同, 如何配置 logstash 来收集这个不同格式的日志。

## 语法

### 区段 section

logstash，使用 `{}` 定义区域。区域内可以定义多个插件域，插件区域内可以定义键值对。

```ruby
input {
	stdin {}
	syslog {}
}
```

### 数据类型

包括：布尔值，字符串，数值，数组，哈希

``` ruby

# bool

debug => true

# string

host => "hostname"

# number

port => 5144

# array

match => ["datetime", "unix", "ISO8601"]

# hash

options => {
	key1 => "value1",
	key2 => "value2"
}

```

### 字段引用 field reference

假如存在一个对象，如  `Logstash::Event`，如果要获取 logstash 配置中使用的值，可使用 `[]` 获取，如果存在对象嵌套则每层都使用 `[]` 获取即可。

```ruby
# 获取 event 

event => [Logstash][Event]

# 获取第一个 geoip

ip => [geoip][location][0]

# 倒序下标，获取数组最后一个元素的值

lastIp => [geoip][location][-1]

# 变量内插

someEvent => "the longitude is %{[geoip][location][0]}"
```

### 条件判断

- `==` （等于）， `!=` （不等于）， `<` （小于）， `>` （大于）， `<=` （小于等于）， `>=` （大于等于）
- `=~` （匹配正则）， `!~`（不匹配正则）
- `in`（包含）, `not in` （不包含）
- `and`（与）, `or` （或），`nand` （非与）, `xor` （非或）
- `()`（复合表达式）, `!()` （对复合表达式结果取反）

- 例子
```ruby
if "_grokparsefailure" not in [tags] {

} else if [status] !~ /^2\d\d/ or ( [url] == "/noc.gif" nand [geoip][city] != "beijing" ) {

} else {

}
```

## 插件

### 输入插件 Input 

#### file 

File：读取文件

Logstash 使用一个名叫 _FileWatch_ 的 Ruby Gem 库来监听文件变化。这个库支持 glob 展开文件路径，而且会记录一个叫 _.sincedb_ 的数据库文件来跟踪被监听的日志文件的当前读取位置。

_sincedb_ 文件记录了每个被监听文件的 inode, major number, minor number 和 pos。

> 配置示例

```ruby
input {
	file {
		path => ["var/log/*.log", "var/log/message"]
		type => "system"
		start_position => "beginning"
	}
}
```

>其他的配置项

- `discover_interval`：每隔多少秒检查一下，监听 path 路径下是否有新文件，默认 15 秒；

- `exclude`：排除不被监听的文件，跟  path 一样支持 glob 展开；

- `close_older`：一个已经监听中的文件，如果超过这个值的时间内没有更新内容，就关闭监听它的文件句柄。默认是 3600 秒，即一小时；

- `ignore_older`：在每次检查文件列表的时候，如果一个文件的最后修改时间超过这个值，就忽略这个文件。默认是 86400 秒，即一天；

- `sincedb_path`：配置自定义的 sincedb 文件的位置，Linux 默认路径：`$HOME/.sincedb`；

- `sincedb_write_interval`：每隔多久写一次 sincedb 文件，默认 15 秒；

- `stat_interval`：每隔多久检查一次被监听文件状态（是否有更新），默认 1 秒；

- `start_position`：logstash 从什么位置开始读取文件数据，默认是结束位置，也就是说 logstash 进程会以类似 `tail -F` 的形式运行。如果你是要导入原有数据，把这个设定改成 "beginning"，logstash 进程就从头开始读取，类似 `less +F` 的形式运行。

> 注意点

1. 不支持的 path 写法：`path => "/path/to/%{+yyyy/MM/dd/hh}.log"`，但可以写成通配符形式：`/path/to/**/*.log`，而且路径必须是绝对路径；
2. `start_position` 仅在该文件从未被监听过的时候起作用。如果 sincedb 文件中已经有这个文件的 inode 记录了，那么 logstash 依然会从记录过的 pos 开始读取数据。所以重复测试的时候每回需要删除 sincedb 文件(官方博客上提供了[另一个巧妙的思路](https://www.elastic.co/blog/logstash-configuration-tuning)：将 `sincedb_path` 定义为 `/dev/null`，则每次重启自动从头开始读)。

#### Stdin 

stdin：标准输入

> 配置示例

```ruby
input {
    stdin {
        add_field => {"key" => "value"}
        codec => "plain"
        tags => ["add"]
        type => "std"
    }
}

output {
    if [type] == "std" {
        stdout {}
    }
}

```

- `type`：特殊字段，可以在 input 段中来标记事件类型；
- `tags`：在数据处理过程中，由具体的插件来添加或者删除。

#### Syslog 

`syslog` 可能是运维领域最流行的数据传输协议了。当你想从设备上收集系统日志的时候，syslog 应该会是你的第一选择。

> 配置示例

```ruby
input {
  syslog {
    port => "514"
  }
}
```

Logstash 是用 `UDPSocket`, `TCPServer` 和 `LogStash::Filters::Grok` 来实现 `LogStash::Inputs::Syslog` 的。所以你其实可以直接用 logstash 配置实现一样的效果：

```ruby
input {
  tcp {
    port => "8514"
  }
}
filter {
  grok {
    match => ["message", "%{SYSLOGLINE}" ]
  }
  syslog_pri { }
}
```

**性能原因：强烈建议使用 `LogStash::Inputs::TCP` 和 `LogStash::Filters::Grok` 配合实现同样的 syslog 功能！**

#### TCP 

未来你可能会用 Redis 服务器或者其他的消息队列系统来作为 logstash broker 的角色。不过 Logstash 其实也有自己的 TCP/UDP 插件，在临时任务的时候，也算能用，尤其是测试环境。

_小贴士：_ 虽然 `LogStash::Inputs::TCP` 用 Ruby 的 `Socket` 和 `OpenSSL` 库实现了高级的 SSL 功能，但 Logstash 本身只能在 `SizedQueue` 中缓存 20 个事件。这就是我们建议在生产环境中换用其他消息队列的原因。

> 配置示例

```ruby
input {
    tcp {
        port => 8888
        mode => "server"
        ssl_enable => false
        "type" => "web"
    }
}

output {
    if [type] == "web" {
        stdout {}
    }
}

```

> 场景

目前来看，`LogStash::Inputs::TCP` 最常见的用法就是配合 `nc` 命令导入旧数据。在启动 logstash 进程后，在另一个终端运行如下命令即可导入数据：

```
echo "hello world" | nc 127.0.0.1 8888
```

这种做法比用 `LogStash::Inputs::File` 好，因为当 nc 命令结束，我们就知道数据导入完毕了。而用 input/file 方式，logstash 进程还会一直等待新数据输入被监听的文件，不能直接看出是否任务完成了。

### 编码插件 Codec 

Codec，Coder 和 Decoder 两个单词的缩写，可以用来处理输入过程中的文本。

Logstash 的数据流：`input | decode | filter | encode | output`。

#### Json

降低 logstash 过滤器的 CPU 负载消耗 ：**直接输入预定义好的 JSON 数据，这样就可以省略掉 filter/grok 配置！** ，现在有专门的 _codec_ 设置。

> 配置示例

- nginx 配置文件，nignx.conf。

```conf
logformat json '{"@timestamp":"$time_iso8601",'
               '"@version":"1",'
               '"host":"$server_addr",'
               '"client":"$remote_addr",'
               '"size":$body_bytes_sent,'
               '"responsetime":$request_time,'
               '"domain":"$host",'
               '"url":"$uri",'
               '"status":"$status"}';
               
access_log /var/log/nginx/access.log_json json;
```

**注意**：在 `$request_time` 和 `$body_bytes_sent` 变量两头没有双引号 `"`，这两个数据在 JSON 里应该是数值类型！

- logstash input/file 段配置

```ruby
input {
    file {
        path => "/var/log/nginx/access.log_json"
        codec => "json"
    }
}
```

- 输出结果

```
{
      "@timestamp" => "2014-03-21T18:52:25.000+08:00",
        "@version" => "1",
            "host" => "raochenlindeMacBook-Air.local",
          "client" => "123.125.74.53",
            "size" => 8096,
    "responsetime" => 0.04,
          "domain" => "www.domain.com",
             "url" => "/path/to/file.suffix",
          "status" => "200"
}
```

**注意：** 对于一个 web 服务器的访问日志，看起来已经可以很好的工作了。不过如果 Nginx 是作为一个代理服务器运行的话，访问日志里有些变量，比如说 `$upstream_response_time`，可能不会一直是数字，它也可能是一个 `"-"` 字符串！这会直接导致 logstash 对输入数据验证报异常。

**解决方法：** 日志格式中统一记录为字符串格式(即都带上双引号 `"`)，然后再在 logstash 中用 `filter/mutate` 插件来变更应该是数值类型的字符字段的值类型。

#### 合并多行数据 Multiline

应用程序日志有可能会打印出多行堆栈信息，其实他们都属于同一个 log。这是可以借助  _codec/multiline_  插件来完成多行日志的合并。

> 配置示例

```ruby
input {
    stdin {
        codec => multiline {
            pattern => "^\["
            negate => true
            what => "previous"
        }
        "type" => "web"
    }
}


output {
    if [type] == "web" {
        stdout {
        	codec => rubydebug
        }
    }
}
```

- 输入

```
[Aug/08/08 14:54:03] hello world
[Aug/08/09 14:54:04] hello logstash
    hello best practice
    hello raochenlin
[Aug/08/10 14:54:05] the end
```

- 输出
按照匹配模式 `^\[`，输出 3 行 mssage。

```
{
    "@timestamp" => 2024-04-23T07:28:23.604Z,
      "@version" => "1",
          "type" => "web",
       "message" => "[Aug/08/10 14:54:05] the end \\r\\n",
          "host" => "localhost.localdomain"
}
{
          "tags" => [
        [0] "multiline"
    ],
      "@version" => "1",
          "type" => "web",
       "message" => "[Aug/08/09 14:54:04] hello logstash\n    hello best practice\n    hello raochenlin",
    "@timestamp" => 2024-04-23T07:28:23.605Z,
          "host" => "localhost.localdomain"
}
{
    "@timestamp" => 2024-04-23T07:28:23.604Z,
      "@version" => "1",
          "type" => "web",
       "message" => "[Aug/08/08 14:54:03] hello world",
          "host" => "localhost.localdomain"
}
```

### 过滤器插件 Filter 

丰富的过滤器插件的存在是 logstash 威力如此强大的重要因素。名为过滤器，其实提供的不单单是过滤的功能。在本章我们就会重点介绍几个插件，它们扩展了进入过滤器的原始数据，进行复杂的逻辑处理，甚至可以无中生有的添加新的 logstash 事件到后续的流程中去！

#### date 时间处理

`filters/date`：时间处理插件，用来转换日志记录中的时间字符串，变成 `LogStash::Timestamp` 对象，然后转存到 `@timestamp` 字段里。

**注意：因为在稍后的 outputs/elasticsearch 中常用的 `%{+YYYY.MM.dd}` 这种写法必须读取 `@timestamp` 数据，所以一定不要直接删掉这个字段保留自己的字段，而是应该用 filters/date 转换后删除自己的字段！**

`filters/date` 插件支持五种时间格式：

> **IOS 8601**

类似 "2011-04-19T 03:44:01.103Z" 这样的格式。具体Z后面可以有 "08:00"也可以没有，".103"这个也可以没有。常用场景里来说，Nginx 的 _log_format_ 配置里就可以使用 `$time_iso8601` 变量来记录请求时间成这种格式。

> **UNIX**

UNIX 时间戳格式，记录从 1970 起到现在的总秒数。场景：Squid

> **UNIX_MS**

UNIX 时间戳格式，记录从 1970 起到现在的总毫秒数。场景：JavaScript

> **TAI64N**

极少见，格式：`@4000000052f88ea32489532c`。场景：qmail

> **Joda-Time** 库

Logstash 内部使用了 Java 的 Joda 时间库来作时间处理。所以我们可以使用 Joda 库所支持的时间格式来作具体定义。Joda 时间格式参考网络资源。


很多中国用户经常提一个问题：为什么 @timestamp 比我们晚了 8 个小时？怎么修改成北京时间？

其实，Elasticsearch 内部，对时间类型字段，是**统一采用 UTC 时间，存成 long 长整形数据的**！对日志统一采用 UTC 时间存储，是国际安全/运维界的一个通识——欧美公司的服务器普遍广泛分布在多个时区里——不像中国，地域横跨五个时区却只用北京时间。

对于页面查看，ELK 的解决方案是在 Kibana 上，读取浏览器的当前时区，然后在页面上转换时间内容的**显示**。

#### Grok 正则捕获

Grok 正则和普通正则是有所区别的，Grok 正则表达式通常是为了方便解析日志数据而设计的，提供了一种更易读和易用的方式来定义匹配模式。普通正则表达式是一种通用的正则表达式语法，用于在文本中匹配特定模式的字符串。普通正则表达式通常更灵活和强大，可以用于各种文本处理和匹配需求。简单来说：**Grok 是普通正则的封装，提供更为简洁和强大的匹配模式。**

Grok 的一些用法：[Logstash日志解析的Grok模式详解 - jiayou111 - 博客园](https://www.cnblogs.com/even160941/p/16326986.html)

> 示例 1 （普通正则匹配）

输入，"begin 123.456 end"。其中 message 的匹配模式如下，其含义是匹配到正则的值赋予给 request_time 字段。

```ruby
input {stdin{}}
filter {
    grok {
        match => {
            "message" => "\s+(?<request_time>\d+(?:\.\d+)?)\s+"
        }
    }
}
output {stdout{codec => rubydebug}}
```

输出：

```ruby
{
         "message" => "begin 123.456 end",
        "@version" => "1",
      "@timestamp" => "2014-08-09T11:55:38.186Z",
            "host" => "raochenlindeMacBook-Air.local",
    "request_time" => "123.456"
}
```

> 示例 2 （Grok 表达式语法）

Grok 表达式通常使用方式：

1. 定义 Grok 表达式语法

比如，定义变量 `USERNAME` 关联的正则表达式，定义变量 `USER` 关联的变量值 `%{USERNAME}` 。
这是官方内置的一些 Pattern，可参考写法：[logstash-patterns-core/patterns at main · logstash-plugins/logstash-patterns-core · GitHub](https://github.com/logstash-plugins/logstash-patterns-core/tree/main/patterns)

```
USERNAME [a-zA-Z0-9._-]+
USER %{USERNAME}
```

2. 使用 Grok 语法

使用已定义的变量值 `PATTERN_NAME`，匹配后赋值到变量名 `capture_name`，并且使用指定的数据类型 `data_type`。目前 `data_type` 只支持两个值：`int` 和 `float`。

```
%{PATTERN_NAME:capture_name:data_type}
```

输入：

```ruby
filter {
    grok {
        match => {
            "message" => "%{WORD} %{NUMBER:request_time:float} %{WORD}"
        }
    }
}
```

输出：

```ruby
{
         "message" => "begin 123.456 end",
        "@version" => "1",
      "@timestamp" => "2014-08-09T12:23:36.634Z",
            "host" => "raochenlindeMacBook-Air.local",
    "request_time" => 123.456
}
```

##### 最佳实践

可以使用  `patterns_dir` 来指定管理 `Pattern` 的文件。

如果你把 "message" 里所有的信息都 grok 到不同的字段了，数据实质上就相当于是重复存储了两份。所以你可以用 `remove_field` 参数来删除掉 _message_ 字段，或者用 `overwrite` 参数来重写默认的 _message_ 字段，只保留最重要的部分。

如：

```ruby
filter {
    grok {
        patterns_dir => ["/path/to/your/own/patterns"]
        match => {
            "message" => "%{SYSLOGBASE} %{DATA:message}"
        }
        overwrite => ["message"]
    }
}
```

> 多行匹配

在和 _codec/multiline_ 搭配使用的时候，需要注意一个问题，grok 正则和普通正则一样，默认是不支持匹配回车换行的。就像你需要 `=~ //m` 一样也需要单独指定，具体写法是在表达式开始位置加 `(?m)` 标记。如下所示：

```ruby
match => {
    "message" => "(?m)\s+(?<request_time>\d+(?:\.\d+)?)\s+"
}
```

> 多项选择

针对同一个字段，**有不同的日志格式定义时**，可以使用多项的 Grok 匹配来匹配同一个字段，使用 `[]` 定义多个 Grok 表达式。logstash 会按照这个定义次序依次尝试匹配，到匹配成功为止。（其实亦可使用分隔符 | 来区分不同的日志格式隔离，可读性相当来说差了点）

```ruby
match => [
    "message", "(?<request_time>\d+(?:\.\d+)?)",
    "message", "%{SYSLOGBASE} %{DATA:message}",
    "message", "(?m)%{WORD}"
]
```

Kibana 自带 Grok Debugger 调试器，每个表达式都应该使用 debugger 来调试一下。

#### Dissect

Dissect 是另外的一种日志切割插件，实现功能效果类似于 Grok，当日志格式有比较简明的分隔标志位，而且重复性较大的时候，我们可以使用 dissect 插件更快的完成解析工作。

Grok 使用正则方式匹配在性能和资源上损耗更大，Dissect 相当来说在资源使用和速度上更优。使用范例参考：[filter-grok,dissect匹配数据 - dance\_man - 博客园](https://www.cnblogs.com/dance-walter/p/10325190.html)

```ruby
filter {
    dissect {
        mapping => {
            "message" => "%{ts} %{+ts} %{+ts} %{src} %{} %{prog}[%{pid}]: %{msg}"
        }
        convert_datatype => {
            pid => "int"
        }
    }
}
```

#### GeoIP 

`GeoIP` 可以根据来源的 IP 来生成一些地域的信息。

配置：

```ruby
input {
    stdin {}
}

filter {
    geoip {
        source => "message"
    }
}

output {
	stdout {}
}
```

输入：

```
183.2.172.185
```

输出：

```ruby
{
         "geoip" => {
         "country_code2" => "CN",
                    "ip" => "183.2.172.185",
              "timezone" => "Asia/Shanghai",
           "region_name" => "Guangdong",
              "latitude" => 23.1167,
         "country_code3" => "CN",
             "longitude" => 113.25,
        "continent_code" => "AS",
          "country_name" => "China",
           "region_code" => "GD",
              "location" => {
            "lat" => 23.1167,
            "lon" => 113.25
        }
    },
      "@version" => "1",
          "host" => "localhost.localdomain",
       "message" => "183.2.172.185",
    "@timestamp" => 2024-05-07T05:54:13.516Z
}
```

如果仅需要输出特定的字段，可以配置 `fields`  选项指定自己所需要的。

```ruby
filter {
    geoip {
        fields => [
            "city_name",
            "continent_code",
            "country_code2",
            "country_code3",
            "country_name",
            "dma_code",
            "ip",
            "latitude",
            "longitude",
            "postal_code",
            "region_name",
            "timezone"
        ]
    }
}
```

其中 `geoip.location`  也是通过 `latitude`  和  `longitude` 生成的，可能存在冗余，亦可通过 `remove_field` 来去掉指定的字段。

```ruby
filter { 
    geoip { 
        fields => [
            "city_name",
            "country_code2",
            "country_name",
            "latitude",
            "longitude",
            "region_name"
        ] 
        
        remove_field => [
            "[geoip][latitude]",
            "[geoip][longitude]"
        ]
    }
}
```

**注意：** 输入的 IP 需要在 geoip 库内只存有公共网络上的 IP 信息，查询不到结果的，会直接返回 null，而 logstash 的 geoip 插件对 null 结果的处理是：**不生成对应的 geoip.字段。**

所以读者在测试时，如果使用了诸如 127.0.0.1, 172.16.0.1, 192.168.0.1, 10.0.0.1 等内网地址，会发现没有对应输出！

#### Json
我们知道编码插件 codec 中也有 Json 编码，当一些日志是符合结构时，此时解析出来的 json 结构就需要在 filter 阶段来处理。如日志结构：

```
2024-05-07T05 [INFO] logger - {"a":"1","b":"2","c":{"abc":123,"bcd":345}}
```

**注意数据流程结构**：`input | decode | filter | encode | output`。

> 示例：

配置 （多层结构的话，删掉 `target` 配置即可）

```ruby
input {
    stdin {}
}

filter {
    json {
        source => "message"
        target => "jsoncontent"
    }
}

output {
  stdout {}
}
```

输入

```
{"a":"1","b":"2","c":{"abc":123,"bcd":345}}
```

输出

```

{
       "@version" => "1",
    "jsoncontent" => {
        "a" => "1",
        "b" => "2",
        "c" => {
            "bcd" => 345,
            "abc" => 123
        }
    },
     "@timestamp" => 2024-05-08T08:58:10.090Z,
           "host" => "localhost.localdomain",
        "message" => "{\"a\":\"1\",\"b\":\"2\",\"c\":{\"abc\":123,\"bcd\":345}}"
}
```

#### Mutate 数据修改

_filters/mutate_ 是 Logstash 的重要插件，主要提供数据类型转换、字符串处理和字段处理等功能。

> 类型转换 

_filters/mutate_ 可以设置转换类型包括：`integer`，`float` 和 `string`。

```ruby
filter {
    mutate {
        convert => ["request_time", "float"]
    }
}
```

**注意：mutate 除了转换简单的字符值，还支持对数组类型的字段进行转换，即将 `["1","2"]` 转换成 `[1,2]`。但不支持对哈希类型的字段做类似处理。有这方面需求的可以采用稍后讲述的 filters/ruby 插件完成。**

##### 字符串处理 

- **gsub**

仅对字符串类型字段有效，指定字符串类型进行替换，支持正则类型。

```
filter {
    mutate {
    	# 将字段 urlparams 的 value，使用正则匹配，匹配到的值替换成 _
        gsub => ["urlparams", "[\\?#]", "_"]
    }
}
```

- **split** 

指定分隔符来分隔字符串。

```
filter {
    mutate {
        split => ["message", "|"]
    }
}
```

示例：

```
# 输入
123|321|adfd|dfjld*=123

# 输出
{
    "message" => [
        [0] "123",
        [1] "321",
        [2] "adfd",
        [3] "dfjld*=123"
    ],
    "@version" => "1",
    "@timestamp" => "2014-08-20T15:58:23.120Z",
    "host" => "raochenlindeMacBook-Air.local"
}
```

- **join** 

仅对数组类型字段有效，在 `split` 割切的基础再 `join` 回去。

```
filter {
    mutate {
        split => ["message", "|"]
    }
    mutate {
        join => ["message", ","]
    }
}
```

示例：

```
# 输入
123|321|adfd|dfjld*=123

# 输出
{
	"message" => "123,321,adfd,dfjld*=123",
    "@version" => "1",
    "@timestamp" => "2014-08-20T15:58:23.120Z",
    "host" => "raochenlindeMacBook-Air.local"
}
```

- merge

合并两个数组或者哈希字段。依然在之前 split 的基础上继续：

```
filter {
    mutate {
        split => ["message", "|"]
    }
    mutate {
        merge => ["message", "message"]
    }
}
```

示例：

```
{
       "message" => [
        [0] "123",
        [1] "321",
        [2] "adfd",
        [3] "dfjld*=123",
        [4] "123",
        [5] "321",
        [6] "adfd",
        [7] "dfjld*=123"
    ],
      "@version" => "1",
    "@timestamp" => "2014-08-20T16:05:53.711Z",
          "host" => "raochenlindeMacBook-Air.local"
}
```

示例 2

```
# 配置

filter {
    mutate {
        split => ["message", "|"]
    }
    mutate {
        merge => ["message", "host"]
    }
}

# 输出
{
       "message" => [
        [0] "123",
        [1] "321",
        [2] "adfd",
        [3] "dfjld*=123",
        [4] "raochenlindeMacBook-Air.local"
    ],
      "@version" => "1",
    "@timestamp" => "2014-08-20T16:07:53.533Z",
          "host" => [
        [0] "raochenlindeMacBook-Air.local"
    ]
}
```

- strip
- lowercase
- uppercase

##### 字段处理

- rename 

重命名某个字段，如果目的字段已存在，会被覆盖；

```
filter {
    mutate {
        rename => ["syslog_host", "host"]
    }
}
```

- update

更新某个字段的内容。如果字段不存在，不会新建。

- replace

作用和 update 类似，但是当字段不存在的时候，它会起到 `add_field` 参数一样的效果，自动添加新的字段。

##### 内部执行次序

需要注意的是，filter/mutate 内部是有执行次序的。其次序如下：

```
    rename(event) if @rename
    update(event) if @update
    replace(event) if @replace
    convert(event) if @convert
    gsub(event) if @gsub
    uppercase(event) if @uppercase
    lowercase(event) if @lowercase
    strip(event) if @strip
    remove(event) if @remove
    split(event) if @split
    join(event) if @join
    merge(event) if @merge

    filter_matched(event)
```

而 `filter_matched` 这个 filters/base.rb 里继承的方法也是有次序的。

```
  @add_field.each do |field, value|
  end
  @remove_field.each do |field|
  end
  @add_tag.each do |tag|
  end
  @remove_tag.each do |tag|
  end
```

