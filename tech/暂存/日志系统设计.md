1. 技术选型
before: elk + kinaba, 内存贵
after: lk + doris 


OLAP 实时统计数据库


ELK stack 中文指南：[hello world · ELKstack 中文指南](https://elkguide.elasticsearch.cn/logstash/get-start/hello-world.html)

[Spring Cloud 系列之 Sleuth 链路追踪（一） - 哈喽沃德先生 - 博客园](https://www.cnblogs.com/mrhelloworld/p/sleuth1.html)

[Spring Cloud Sleuth 2.0概要使用说明 - BTStream's Blog](https://blog.btstream.net/post/2019-01-14-spring-cloud-sleuth-2.0%E6%A6%82%E8%A6%81%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E/)

基于 clickhouse 的日志系统

[JLog: 来自京东App秒级百G级日志搜集、传输、存储解决方案](https://gitee.com/jd-platform-opensource/jlog?_from=gitee_search)


> 思想

[51 分布式下如何实现统一日志系统？](https://learn.lianglianglee.com/%E4%B8%93%E6%A0%8F/%E5%88%86%E5%B8%83%E5%BC%8F%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E6%88%9845%E8%AE%B2-%E5%AE%8C/51%20%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%8B%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E7%BB%9F%E4%B8%80%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9F%EF%BC%9F.md)

[性能调优之日志打印的坑 - 楼兰胡杨 - 博客园](https://www.cnblogs.com/east7/p/18013052)

> 工具

正则表达式练习：[ttps://regexlearn.com/](https://regexlearn.com/)


# logback
## 相关文档

官方文档：[Documentation](https://logback.qos.ch/documentation.html)
中文文档：  [README | logback](https://logbackcn.gitbook.io/logback)



> 一些坑 

1. 应用服务内压缩日志导致 cpu 过高问题: 不应该在应用服务内执行压缩, 应该将日志文件转移至其他空闲服务器执行压缩.

[技术干货|关于logback日志压缩的那点事 - 知乎](https://zhuanlan.zhihu.com/p/133425425)

## 架构

日志级别：**TRACE** < **DEBUG** < **INFO** < **WARN** < **ERROR**。

⚠️upload failed, check dev console
![[Logback 日志级别.png]]

## 流程

1. 定义 `logback.xml`，维护 root logger，自定义 Appender，filter，level；
2. 获取 logger，`Logger logger = LoggerFactory.getLogger("className or loggerName");`，如果没有找到自定义的 logger，一直往父级查找，直到 root logger，查找到第一个 logger 为止。
3. 使用获取到的 logger，判断日志的优先级时候执行，获取 logger 绑定的 Appender，执行 `doAppend()` 方法，打印日志。


参考文档：[打印日志时 Logback 内部都做了些什么](https://tech.youzan.com/how-logback-print-log/)
## 注意点
1. 定义多个 root 节点

`logback.xml` 中定义多个 root 节点时，后面的 root 节点定义会覆盖前面的定义。具体还需要看解析器的执行。而且这种做法是不规范，会导致 appender 绑定混乱。标准做法是只定义一个 root 节点。


2. 配置 additivity = false

- 子 logger 会默认继承父 logger 的 appender，将它们加入到自己的 Appender 中；除非加上了**additivity="false"**，则不再继承父 logger 的 appender。
- 子 logger 只在自己未定义输出级别的情况下，才会继承父 logger 的输出级别。
- 【**强制】** 避免重复打印日志，浪费磁盘空间，务必在日志配置文件中设置 additivity=false


# Log4j2

官方文档：[Log4j – Overview](https://logging.apache.org/log4j/2.x/manual/index.html)
中文文档：[Log4j2 中文文档 - 欢迎使用 Log4j 2！ | Docs4dev](https://www.docs4dev.com/docs/zh/log4j2/2.x/all/manual-index.html)
github: [GitHub - apache/logging-log4j2: Apache Log4j 2 is a versatile, feature-rich, efficient logging API and backend for Java.](https://github.com/apache/logging-log4j2)

## 博客

1. 日志的执行流程，异步的无锁数据结构 Disruptor
[Log4j2中的同步日志与异步日志 - Ye\_yang - 博客园](https://www.cnblogs.com/yeyang/p/7944906.html)

![[Log4j2日志输出方式同步和异步.png]]



## API

> 获取 logger
-  `org.apache.logging.log4j.spi.AbstractLoggerAdapter#getLogger`

>构建 log 消息
- `org.apache.logging.log4j.message.MessageFactory2`
- `org.apache.logging.log4j.message.MessageFactory`

## 使用

> 静态还是非静态

建议使用静态的 Logger，因为创建 Logger 很昂贵，而 Logger 被关联的 LoggerContext 关联，不会将其回收。
[Log4j2 中文文档 - Usage | Docs4dev](https://www.docs4dev.com/docs/zh/log4j2/2.x/all/manual-usage.html)


> 关于性能

![[log4j2打印消耗性能的操作.png]]



# Logstash

> 疑问:

1. Exception, run, request, response log 输出的格式不同, 如何配置 logstash 来收集这个不同格式的日志。

## 语法

### 区段 section

logstash，使用 `{}` 定义区域。区域内可以定义多个插件域，插件区域内可以定义键值对。

```ruby
input {
	stdin {}
	syslog {}
}
```

### 数据类型

包括：布尔值，字符串，数值，数组，哈希

``` ruby

# bool

debug => true

# string

host => "hostname"

# number

port => 5144

# array

match => ["datetime", "unix", "ISO8601"]

# hash

options => {
	key1 => "value1",
	key2 => "value2"
}

```

### 字段引用 field reference

假如存在一个对象，如  `Logstash::Event`，如果要获取 logstash 配置中使用的值，可使用 `[]` 获取，如果存在对象嵌套则每层都使用 `[]` 获取即可。

```ruby
# 获取 event 

event => [Logstash][Event]

# 获取第一个 geoip

ip => [geoip][location][0]

# 倒序下标，获取数组最后一个元素的值

lastIp => [geoip][location][-1]

# 变量内插

someEvent => "the longitude is %{[geoip][location][0]}"
```

### 条件判断

- `==` （等于）， `!=` （不等于）， `<` （小于）， `>` （大于）， `<=` （小于等于）， `>=` （大于等于）
- `=~` （匹配正则）， `!~`（不匹配正则）
- `in`（包含）, `not in` （不包含）
- `and`（与）, `or` （或），`nand` （非与）, `xor` （非或）
- `()`（复合表达式）, `!()` （对复合表达式结果取反）

- 例子
```ruby
if "_grokparsefailure" not in [tags] {

} else if [status] !~ /^2\d\d/ or ( [url] == "/noc.gif" nand [geoip][city] != "beijing" ) {

} else {

}
```

## 插件

### 输入插件 Input 

#### file 

File：读取文件

Logstash 使用一个名叫 _FileWatch_ 的 Ruby Gem 库来监听文件变化。这个库支持 glob 展开文件路径，而且会记录一个叫 _.sincedb_ 的数据库文件来跟踪被监听的日志文件的当前读取位置。

_sincedb_ 文件记录了每个被监听文件的 inode, major number, minor number 和 pos。

> 配置示例

```ruby
input {
	file {
		path => ["var/log/*.log", "var/log/message"]
		type => "system"
		start_position => "beginning"
	}
}
```

>其他的配置项

- `discover_interval`：每隔多少秒检查一下，监听 path 路径下是否有新文件，默认 15 秒；

- `exclude`：排除不被监听的文件，跟  path 一样支持 glob 展开；

- `close_older`：一个已经监听中的文件，如果超过这个值的时间内没有更新内容，就关闭监听它的文件句柄。默认是 3600 秒，即一小时；

- `ignore_older`：在每次检查文件列表的时候，如果一个文件的最后修改时间超过这个值，就忽略这个文件。默认是 86400 秒，即一天；

- `sincedb_path`：配置自定义的 sincedb 文件的位置，Linux 默认路径：`$HOME/.sincedb`；

- `sincedb_write_interval`：每隔多久写一次 sincedb 文件，默认 15 秒；

- `stat_interval`：每隔多久检查一次被监听文件状态（是否有更新），默认 1 秒；

- `start_position`：logstash 从什么位置开始读取文件数据，默认是结束位置，也就是说 logstash 进程会以类似 `tail -F` 的形式运行。如果你是要导入原有数据，把这个设定改成 "beginning"，logstash 进程就从头开始读取，类似 `less +F` 的形式运行。

> 注意点

1. 不支持的 path 写法：`path => "/path/to/%{+yyyy/MM/dd/hh}.log"`，但可以写成通配符形式：`/path/to/**/*.log`，而且路径必须是绝对路径；
2. `start_position` 仅在该文件从未被监听过的时候起作用。如果 sincedb 文件中已经有这个文件的 inode 记录了，那么 logstash 依然会从记录过的 pos 开始读取数据。所以重复测试的时候每回需要删除 sincedb 文件(官方博客上提供了[另一个巧妙的思路](https://www.elastic.co/blog/logstash-configuration-tuning)：将 `sincedb_path` 定义为 `/dev/null`，则每次重启自动从头开始读)。

#### Stdin 

stdin：标准输入

> 配置示例

```ruby
input {
    stdin {
        add_field => {"key" => "value"}
        codec => "plain"
        tags => ["add"]
        type => "std"
    }
}

output {
    if [type] == "std" {
        stdout {}
    }
}

```

- `type`：特殊字段，可以在 input 段中来标记事件类型；
- `tags`：在数据处理过程中，由具体的插件来添加或者删除。

#### Syslog 

`syslog` 可能是运维领域最流行的数据传输协议了。当你想从设备上收集系统日志的时候，syslog 应该会是你的第一选择。

> 配置示例

```ruby
input {
  syslog {
    port => "514"
  }
}
```

Logstash 是用 `UDPSocket`, `TCPServer` 和 `LogStash::Filters::Grok` 来实现 `LogStash::Inputs::Syslog` 的。所以你其实可以直接用 logstash 配置实现一样的效果：

```ruby
input {
  tcp {
    port => "8514"
  }
}
filter {
  grok {
    match => ["message", "%{SYSLOGLINE}" ]
  }
  syslog_pri { }
}
```

**性能原因：强烈建议使用 `LogStash::Inputs::TCP` 和 `LogStash::Filters::Grok` 配合实现同样的 syslog 功能！**

#### TCP 

未来你可能会用 Redis 服务器或者其他的消息队列系统来作为 logstash broker 的角色。不过 Logstash 其实也有自己的 TCP/UDP 插件，在临时任务的时候，也算能用，尤其是测试环境。

_小贴士：_ 虽然 `LogStash::Inputs::TCP` 用 Ruby 的 `Socket` 和 `OpenSSL` 库实现了高级的 SSL 功能，但 Logstash 本身只能在 `SizedQueue` 中缓存 20 个事件。这就是我们建议在生产环境中换用其他消息队列的原因。

> 配置示例

```ruby
input {
    tcp {
        port => 8888
        mode => "server"
        ssl_enable => false
        "type" => "web"
    }
}

output {
    if [type] == "web" {
        stdout {}
    }
}

```

> 场景

目前来看，`LogStash::Inputs::TCP` 最常见的用法就是配合 `nc` 命令导入旧数据。在启动 logstash 进程后，在另一个终端运行如下命令即可导入数据：

```
echo "hello world" | nc 127.0.0.1 8888
```

这种做法比用 `LogStash::Inputs::File` 好，因为当 nc 命令结束，我们就知道数据导入完毕了。而用 input/file 方式，logstash 进程还会一直等待新数据输入被监听的文件，不能直接看出是否任务完成了。

### 编码插件 Codec 

Codec，Coder 和 Decoder 两个单词的缩写，可以用来处理输入过程中的文本。

Logstash 的数据流：`input | decode | filter | encode | output`。

#### Json

降低 logstash 过滤器的 CPU 负载消耗 ：**直接输入预定义好的 JSON 数据，这样就可以省略掉 filter/grok 配置！** ，现在有专门的 _codec_ 设置。

> 配置示例

- nginx 配置文件，nignx.conf。

```conf
logformat json '{"@timestamp":"$time_iso8601",'
               '"@version":"1",'
               '"host":"$server_addr",'
               '"client":"$remote_addr",'
               '"size":$body_bytes_sent,'
               '"responsetime":$request_time,'
               '"domain":"$host",'
               '"url":"$uri",'
               '"status":"$status"}';
               
access_log /var/log/nginx/access.log_json json;
```

**注意**：在 `$request_time` 和 `$body_bytes_sent` 变量两头没有双引号 `"`，这两个数据在 JSON 里应该是数值类型！

- logstash input/file 段配置

```ruby
input {
    file {
        path => "/var/log/nginx/access.log_json"
        codec => "json"
    }
}
```

- 输出结果

```
{
      "@timestamp" => "2014-03-21T18:52:25.000+08:00",
        "@version" => "1",
            "host" => "raochenlindeMacBook-Air.local",
          "client" => "123.125.74.53",
            "size" => 8096,
    "responsetime" => 0.04,
          "domain" => "www.domain.com",
             "url" => "/path/to/file.suffix",
          "status" => "200"
}
```

**注意：** 对于一个 web 服务器的访问日志，看起来已经可以很好的工作了。不过如果 Nginx 是作为一个代理服务器运行的话，访问日志里有些变量，比如说 `$upstream_response_time`，可能不会一直是数字，它也可能是一个 `"-"` 字符串！这会直接导致 logstash 对输入数据验证报异常。

**解决方法：** 日志格式中统一记录为字符串格式(即都带上双引号 `"`)，然后再在 logstash 中用 `filter/mutate` 插件来变更应该是数值类型的字符字段的值类型。

#### 合并多行数据 Multiline

应用程序日志有可能会打印出多行堆栈信息，其实他们都属于同一个 log。这是可以借助  _codec/multiline_  插件来完成多行日志的合并。

> 配置示例

```ruby
input {
    stdin {
        codec => multiline {
            pattern => "^\["
            negate => true
            what => "previous"
        }
        "type" => "web"
    }
}


output {
    if [type] == "web" {
        stdout {
        	codec => rubydebug
        }
    }
}
```

- 输入

```
[Aug/08/08 14:54:03] hello world
[Aug/08/09 14:54:04] hello logstash
    hello best practice
    hello raochenlin
[Aug/08/10 14:54:05] the end
```

- 输出
按照匹配模式 `^\[`，输出 3 行 mssage。

```
{
    "@timestamp" => 2024-04-23T07:28:23.604Z,
      "@version" => "1",
          "type" => "web",
       "message" => "[Aug/08/10 14:54:05] the end \\r\\n",
          "host" => "localhost.localdomain"
}
{
          "tags" => [
        [0] "multiline"
    ],
      "@version" => "1",
          "type" => "web",
       "message" => "[Aug/08/09 14:54:04] hello logstash\n    hello best practice\n    hello raochenlin",
    "@timestamp" => 2024-04-23T07:28:23.605Z,
          "host" => "localhost.localdomain"
}
{
    "@timestamp" => 2024-04-23T07:28:23.604Z,
      "@version" => "1",
          "type" => "web",
       "message" => "[Aug/08/08 14:54:03] hello world",
          "host" => "localhost.localdomain"
}
```